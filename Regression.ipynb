{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression and its Evaluation Assignment\n"
      ],
      "metadata": {
        "id": "RUQxPUJCTH-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   What is Simple Linear Regression ?\n",
        "\n",
        "   --> Simple Linear Regression (SLR) is a statistical and machine-learning\n",
        "       method used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line. Or simply we say that it explains how a dependent variable changes when one independent variable changes.\n",
        "\n",
        "       Formula  \n",
        "                 Y = β0 + β1​X + ε\n",
        "       where\n",
        "             Y → Dependent variable (output)\n",
        "             X → Independent variable (input)\n",
        "             β₀ (Intercept) → Value of Y when X = 0\n",
        "             β₁ (Slope) → Change in Y for a one-unit change in X\n",
        "             ε (Error term) → Random error   \n",
        "\n",
        "       The main purpose are :-    \n",
        "             1. To understand relationship between variables\n",
        "             2. To predict outcomes\n",
        "             3. To identify trends\n",
        "\n",
        "       Its main assumptions are :-   \n",
        "             1. Linear relationship between X and Y\n",
        "             2. Errors are independent\n",
        "             3. Constant variance of Errors\n",
        "\n",
        "\n",
        "2.    What are the key assumptions of Simple Linear Regression?  \n",
        "\n",
        "   -->  The key assumptions of Simple Linear Regression are ;\n",
        "      1. Linearity\n",
        "      \n",
        "      a. The relationship between X (independent) and Y (dependent) is linear.\n",
        "      b. Mean of Y changes linearly with X.\n",
        "\n",
        "      2. Independence of Errors\n",
        "\n",
        "        a. Residuals (errors) are independent of each other.\n",
        "        b. No autocorrelation (especially important in time-series data).\n",
        "\n",
        "      3. Homoscedasticity (Constant Variance)\n",
        "\n",
        "        a. Variance of residuals is constant for all values of X.\n",
        "        b. If variance changes → heteroscedasticity  \n",
        "\n",
        "      4. Normality of Errors\n",
        "\n",
        "        a. Residuals are normally distributed with mean zero.\n",
        "        b. Mainly required for hypothesis testing and confidence intervals.  \n",
        "\n",
        "      5. No Outliers or Influential Points\n",
        "\n",
        "        a. Extreme values can disproportionately affect the regression line.  \n",
        "\n",
        "\n",
        "3.    What is heteroscedasticity, and why is it important to address in\n",
        "      regression models?\n",
        "\n",
        "   -->  Heteroscedasticity occurs in a regression model when the variance of\n",
        "        the error terms (residuals) is not constant across all levels of the independent variable(s).  Or its simply means the spread of errors changes as X changes.\n",
        "\n",
        "        It is important to address in regressions models because of the following reason ;\n",
        "\n",
        "        1. Invalid Standard Errors\n",
        "\n",
        "         a. Standard errors become incorrect\n",
        "         b. Leads to unreliable t-tests and F-tests\n",
        "\n",
        "        2. Wrong Hypothesis Testing\n",
        "\n",
        "         a. P-values may be too small or too large\n",
        "            You may:\n",
        "         a. Accept a false relationship\n",
        "         b. Reject a true relationship\n",
        "\n",
        "        3. Inefficient Estimates\n",
        "\n",
        "         a. OLS is no longer the Best Linear Unbiased Estimator (BLUE)\n",
        "\n",
        "        4. Poor Confidence Intervals\n",
        "\n",
        "         a. Confidence intervals become misleading\n",
        "\n",
        "\n",
        "4.    What is Multiple Linear Regression?   \n",
        "\n",
        "   -->  Multiple Linear Regression is a statistical technique used to model the\n",
        "        relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, …, Xₙ) by fitting a linear equation.\n",
        "\n",
        "       Formula\n",
        "                Y = β0 + β1X1 + β2X2 +⋯+βnXn + ε\n",
        "       Where\n",
        "            Y → Dependent variable (output)\n",
        "            X₁, X₂, …, Xₙ → Independent variables (inputs)\n",
        "            β₀ → Intercept\n",
        "            β₁, β₂, …, βₙ → Regression coefficients\n",
        "            ε → Error term      \n",
        "\n",
        "       The Purpose of Multiple Linear Regression are ;\n",
        "\n",
        "        a. Analyze the impact of multiple factors on an outcome\n",
        "        b. Make better predictions than simple linear regression\n",
        "        c. Control for confounding variables   \n",
        "\n",
        "       The main assumptions are ;\n",
        "\n",
        "        a. Linear relationship between Y and each X\n",
        "        b. Independence of errors\n",
        "        c. Homoscedasticity\n",
        "        d. Normality of residuals\n",
        "        e. No multicollinearity among independent variables  \n",
        "\n",
        "\n",
        "5.   What is polynomial regression, and how does it differ from linear\n",
        "     regression?    \n",
        "\n",
        "   -->  Polynomial Regression is a type of regression where the relationship\n",
        "        between the independent variable (X) and dependent variable (Y) is modeled as an n-th degree polynomial.    Although it models curved (non-linear) relationships, it is still considered a linear regression model because it is linear in the parameters (coefficients).         \n",
        "\n",
        "        Formula\n",
        "               Y = β0 + β1X + β2X2 + β3X3 + ⋯ + βnXn + ε\n",
        "         \n",
        "        The Polynomial Regression is different from the Linear Regression in following ways ;\n",
        "         Polynomial Regression ;\n",
        "\n",
        "         a. Relationship is Curved ( Non linear)\n",
        "         b. Flexibility is very high\n",
        "         c. Risk is overfitting ( If degree is too high )\n",
        "         d. Equation ( Y = β0 + β1X + β2X2 + … )\n",
        "         e. Its feature is X, X², X³, …\n",
        "\n",
        "         Linear Regression ;\n",
        "\n",
        "         a. Relationship is shown is straight line.\n",
        "         b. Flexibility is very low\n",
        "         c. Risk is Underfitting\n",
        "         d. Equation ( Y = β0 + β1X )\n",
        "         e. Its feature is only X\n",
        "\n",
        "\n",
        "6.     Implement a Python program to fit a Simple Linear Regression model to\n",
        "       the following sample data:\n",
        "            X = [ 1, 2, 3, 4, 5 ]\n",
        "            Y = [ 2.1, 4.3, 6 Implement.1, 7.9, 10.2 ]\n",
        "       Plot the regression line over the data points.    \n",
        "\n",
        "     --> Python Programme are\n",
        "         \n",
        "         import numpy as np\n",
        "         import pandas as pd\n",
        "         import seaborn as sns\n",
        "         import matplotlib.pyplot as plt\n",
        "\n",
        "         import warnings\n",
        "         warnings.filterwarnings('ignore')\n",
        "\n",
        "         X = np.array([1, 2, 3, 4, 5])\n",
        "         Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "         x_mean = np.mean(X)\n",
        "         y_mean = np.mean(Y)\n",
        "\n",
        "        beta1 = np.sum((X - x_mean) * (Y - y_mean)) / np.sum((X - x_mean) ** 2)\n",
        "        beta0 = y_mean - beta1 * x_mean\n",
        "\n",
        "        Y_pred = beta0 + beta1 * X\n",
        "\n",
        "        plt.scatter(X, Y)\n",
        "        plt.plot(X, Y_pred)\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.title(\"Simple Linear Regression\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Intercept (β0):\", beta0)\n",
        "        print(\"Slope (β1):\", beta1)\n",
        "\n",
        "        OUTPUT\n",
        "\n",
        "        Intercept (β0): 0.18\n",
        "        Slope (β1): 1.98\n",
        "\n",
        "\n",
        "7.    Fit a Multiple Linear Regression model on this sample data:\n",
        "          \n",
        "         Area = [1200, 1500, 1800, 2000]\n",
        "         Rooms = [2, 3, 3, 4]\n",
        "         Price = [250000, 300000, 320000, 370000]\n",
        "\n",
        "         Check for multicollinearity using VIF and report the results. (Include your Python code and output in the code box below)\n",
        "\n",
        "    -->  Python Program are ;\n",
        "\n",
        "         import pandas as pd\n",
        "         import numpy as np\n",
        "         from sklearn.linear_model import LinearRegression\n",
        "         from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "         data = {\n",
        "                'Area': [1200, 1500, 1800, 2000],\n",
        "                'Rooms': [2, 3, 3, 4],\n",
        "                'Price': [250000, 300000, 320000, 370000]\n",
        "                }\n",
        "\n",
        "         df = pd.DataFrame(data)\n",
        "\n",
        "         X = df[['Area', 'Rooms']]\n",
        "         y = df['Price']\n",
        "\n",
        "         model = LinearRegression()\n",
        "         model.fit(X, y)\n",
        "\n",
        "         print(\"Intercept:\", model.intercept_)\n",
        "         print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "         vif_data = pd.DataFrame()\n",
        "         vif_data['Feature'] = X.columns\n",
        "         vif_data['VIF'] = [\n",
        "                         variance_inflation_factor(X.values, i)\n",
        "                     for i in range(X.shape[1])\n",
        "                          ]\n",
        "\n",
        "         print(\"\\nVIF Results:\")\n",
        "         print(vif_data)\n",
        "     \n",
        "         OUTPUT\n",
        "         Intercept: 103157.89\n",
        "         Coefficients: [   63.16 34736.84]\n",
        "\n",
        "         VIF Results:\n",
        "             Feature        VIF\n",
        "         0    Area        127.80\n",
        "         1   Rooms        127.80\n",
        "\n",
        "\n",
        "8.   Implement polynomial regression on the following data:\n",
        "         X = [1, 2, 3, 4, 5]\n",
        "         Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "     Fit a 2nd-degree polynomial and plot the resulting curve. (Include your Python code and output in the code box below)\n",
        "\n",
        "   -->  Python Program are\n",
        "       import numpy as np\n",
        "       import pandas as pd\n",
        "       import seaborn as sns\n",
        "       import matplotlib.pyplot as plt\n",
        "\n",
        "       import warnings\n",
        "       warnings.filterwarnings('ignore')\n",
        "\n",
        "       X = np.array([1, 2, 3, 4, 5])\n",
        "       Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "      Fit a 2nd-degree polynomial: y = ax^2 + bx + c\n",
        "      np.polyfit returns coefficients [a, b, c]\n",
        "\n",
        "      coeffs = np.polyfit(X, Y, 2)\n",
        "      poly_eqn = np.poly1d(coeffs)\n",
        "\n",
        "      X_smooth = np.linspace(min(X), max(X), 100)\n",
        "      Y_smooth = poly_eqn(X_smooth)\n",
        "\n",
        "      plt.scatter(X, Y, color='red', label='Original Data')\n",
        "      plt.plot(X_smooth, Y_smooth, color='blue', label=f'Polynomial Fit (degree 2)')\n",
        "      plt.xlabel('X')\n",
        "      plt.ylabel('Y')\n",
        "      plt.title('2nd-Degree Polynomial Regression')\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.savefig('polynomial_regression.png')\n",
        "\n",
        "      print(f\"Coefficients (a, b, c): {coeffs}\")\n",
        "      print(f\"Equation: y = {coeffs[0]:.2f}x^2 + {coeffs[1]:.2f}x + {coeffs[2]:.2f}\")\n",
        "                      \n",
        "      OUTPUT\n",
        "\n",
        "      Coefficients: a = 0.2, b = 1.94, c = 0.06\n",
        "      Polynomial Equation: y = 0.20x^2 + 1.94x + 0.06\n",
        "\n",
        "\n",
        "9.   Create a residuals plot for a regression model trained on this data  \n",
        "     X = [10, 20, 30, 40, 50]\n",
        "     Y = [15, 35, 40, 50, 65]\n",
        "     Assess heteroscedasticity by examining the spread of residuals. (Include your Python code and output in the code box below.)     \n",
        "\n",
        "   -->  Python Program are\n",
        "      import numpy as np\n",
        "      import pandas as pd\n",
        "      import seaborn as sns\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      import warnings\n",
        "      warnings.filterwarnings('ignore')\n",
        "\n",
        "      X = np.array([10, 20, 30, 40, 50])\n",
        "      Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "      coeffs = np.polyfit(X, Y, 1)\n",
        "      model = np.poly1d(coeffs)\n",
        "\n",
        "      Y_pred = model(X)\n",
        "      residuals = Y - Y_pred\n",
        "\n",
        "      plt.scatter(Y_pred, residuals, color='blue', edgecolor='black', s=100)\n",
        "      plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "      plt.xlabel('Predicted Values ($\\hat{Y}$)')\n",
        "      plt.ylabel('Residuals ($Y - \\hat{Y}$)')\n",
        "      plt.title('Residuals Plot for Heteroscedasticity Assessment')\n",
        "      plt.grid(True, linestyle=':', alpha=0.7)\n",
        "      plt.savefig('residuals_plot.png')\n",
        "\n",
        "      print(f\"Predicted Values: {Y_pred}\")\n",
        "      print(f\"Residuals: {residuals}\")\n",
        "\n",
        "      OUTPUT\n",
        "\n",
        "      The sample size is very small (n=5), the residuals show a relatively consistent spread. Therefore, there is no strong evidence of heteroscedasticity.\n",
        "\n",
        "\n",
        "10.   Imagine you are a data scientist working for a real estate company. You  \n",
        "      need to predict house prices using features like area, number of rooms, and location. However, you detect heteroscedasticity and multicollinearity in your regression model. Explain the steps you would take to address these issues and ensure a robust model.  \n",
        "\n",
        "   --> In real estate modeling, house prices often exhibit heteroscedasticity  \n",
        "       (variability increases with price) and multicollinearity (features like \"Total Area\" and \"Number of Rooms\" are highly correlated). These issues can make your coefficients unreliable and inflate your standard errors.\n",
        "\n",
        "       Here is a step-by-step strategy to address these problems and build a robust model.   \n",
        "\n",
        "       1. Addressing Multicollinearity\n",
        "          Multicollinearity occurs when independent variables are highly correlated, making it difficult for the model to isolate the individual effect of each feature.  \n",
        "          a. VIF analysis\n",
        "          b. Feature Selection\n",
        "          c. Regularization\n",
        "\n",
        "       2. Addressing Heteroscedasticity\n",
        "          In real estate, expensive homes often have much higher price variance than entry-level homes, violating the assumption of constant variance (homoscedasticity).\n",
        "          a. Log Transformation\n",
        "          b. Weighted Least Squares (WLS)\n",
        "          c. Robust Standard Errors\n",
        "\n",
        "       3. Workflow for a Robust Model\n",
        "          To ensure the final model is reliable, I would follow this pipeline:       \n",
        "          a. Diagnostic Phase\n",
        "          b. Transformation Phase\n",
        "          c. Refinement Phase\n",
        "          d. Validation Phase"
      ],
      "metadata": {
        "id": "A7Z0knQBTW_S"
      }
    }
  ]
}